{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500001it [00:02, 193713.99it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import orjson as json\n",
    "from tqdm import tqdm\n",
    "\n",
    "arxiv_path = Path(\"/Users/jm/data/arxiv-metadata-oai-snapshot.json\")\n",
    "dataset = defaultdict(list)\n",
    "max_size = 500000\n",
    "with open(arxiv_path, \"rb\") as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        if i > max_size:\n",
    "            break\n",
    "        loaded_data = json.loads(line)\n",
    "        dataset[\"title\"].append(loaded_data[\"title\"])\n",
    "        dataset[\"abstract\"].append(loaded_data[\"abstract\"])\n",
    "        dataset['id'].append(loaded_data['id'])\n",
    "        dataset['doi'].append(loaded_data['doi'])\n",
    "        dataset['categories'].append(loaded_data['categories'].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x525 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "with plt.style.context(['nature', 'science']):\n",
    "    max_cats = 35\n",
    "    fig, axs = plt.subplots(1, 1, dpi=200)\n",
    "    flat_categories = [y for x in dataset['categories'] for y in x]\n",
    "    counter = Counter(flat_categories)\n",
    "    vals = [x[1] for x in counter.most_common(max_cats)]\n",
    "    cats = [x[0] for x in counter.most_common(max_cats)]\n",
    "    axs.barh(cats, vals)\n",
    "    axs.set_xlabel(\"Count\")\n",
    "    axs.tick_params(labelsize=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "# download the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    300\n",
       "0    300\n",
       "1    300\n",
       "3    300\n",
       "Name: label_gt_cat, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "categories = (\"math\", \"cs\", \"astro\", \"bio\")\n",
    "category_map = {category: i for i, category in enumerate(categories)}\n",
    "\n",
    "\n",
    "def include_datapoint(category):\n",
    "    for cand in categories:\n",
    "        if cand in category:\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "\n",
    "def equal_sampling(df, min_sample):\n",
    "    category_counts = df['label'].value_counts()\n",
    "    cats = category_counts.index\n",
    "    counts = category_counts.values\n",
    "    min_count = min(np.min(counts), min_sample)\n",
    "    concat_dfs = []\n",
    "    for cat in cats:\n",
    "        concat_dfs.append(df[df['label'] == cat].sample(min_count))\n",
    "    return pd.concat(concat_dfs)\n",
    "\n",
    "\n",
    "MIN_EXAMPLES = 30\n",
    "SAMPLE = 5000\n",
    "MAX_LABLES = 6\n",
    "dataset_pd = pd.DataFrame.from_dict(dataset)\n",
    "dataset_pd['label_gt'] = dataset_pd['categories'].apply(lambda x: x[0])\n",
    "dataset_pd['label'] = dataset_pd['label_gt'].apply(include_datapoint)\n",
    "dataset_pd = dataset_pd.dropna()\n",
    "dataset_pd['label_gt_cat'] = dataset_pd['label'].apply(\n",
    "    lambda x: category_map[x])\n",
    "dataset_pd = equal_sampling(dataset_pd, min_sample=300)\n",
    "\n",
    "dataset_pd['label_gt_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_queries, val_queries, train_docs, val_docs, train_labels, val_labels = train_test_split(\n",
    "    dataset_pd[\"id\"].tolist(),\n",
    "    dataset_pd[\"title\"].tolist(),\n",
    "    dataset_pd[\"label_gt_cat\"].tolist(),\n",
    "    test_size=.2)\n",
    "\n",
    "train_encodings = tokenizer(train_queries,\n",
    "                            train_docs,\n",
    "                            truncation=True,\n",
    "                            padding='max_length',\n",
    "                            max_length=MAX_LEN)\n",
    "val_encodings = tokenizer(val_queries,\n",
    "                          val_docs,\n",
    "                          truncation=True,\n",
    "                          padding='max_length',\n",
    "                          max_length=MAX_LEN)\n",
    "\n",
    "\n",
    "class ArxivDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(val[idx])\n",
    "            for key, val in self.encodings.items()\n",
    "        }\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = ArxivDataset(train_encodings, train_labels)\n",
    "val_dataset = ArxivDataset(val_encodings, val_labels)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the transfomer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [02:23<00:00,  9.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 21.021209120750427\n",
      "Epoch 0 accuracy: 0.25833333333333336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:58<00:00,  7.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 20.912245869636536\n",
      "Epoch 1 accuracy: 0.25625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [02:12<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 20.852153420448303\n",
      "Epoch 2 accuracy: 0.278125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [02:06<00:00,  8.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 20.72599196434021\n",
      "Epoch 3 accuracy: 0.278125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:08<01:54,  8.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jm/repos/onenote-utils/notebooks/test_arxiv.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jm/repos/onenote-utils/notebooks/test_arxiv.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jm/repos/onenote-utils/notebooks/test_arxiv.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# train model on batch and return outputs (incl. loss)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jm/repos/onenote-utils/notebooks/test_arxiv.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch_mps)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jm/repos/onenote-utils/notebooks/test_arxiv.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# extract loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jm/repos/onenote-utils/notebooks/test_arxiv.ipynb#X11sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1545\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1540\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1545\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1546\u001b[0m     input_ids,\n\u001b[1;32m   1547\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1548\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1549\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1550\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1551\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1552\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1553\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1554\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1555\u001b[0m )\n\u001b[1;32m   1557\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1559\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    987\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    989\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    990\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    991\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    995\u001b[0m )\n\u001b[0;32m--> 996\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    997\u001b[0m     embedding_output,\n\u001b[1;32m    998\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    999\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1000\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1001\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1002\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1003\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1004\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1005\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1006\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1007\u001b[0m )\n\u001b[1;32m   1008\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1009\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    576\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    577\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    578\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    583\u001b[0m     )\n\u001b[1;32m    584\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 585\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    586\u001b[0m         hidden_states,\n\u001b[1;32m    587\u001b[0m         attention_mask,\n\u001b[1;32m    588\u001b[0m         layer_head_mask,\n\u001b[1;32m    589\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    590\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    591\u001b[0m         past_key_value,\n\u001b[1;32m    592\u001b[0m         output_attentions,\n\u001b[1;32m    593\u001b[0m     )\n\u001b[1;32m    595\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    596\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:472\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    461\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    462\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    470\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    473\u001b[0m         hidden_states,\n\u001b[1;32m    474\u001b[0m         attention_mask,\n\u001b[1;32m    475\u001b[0m         head_mask,\n\u001b[1;32m    476\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    477\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    478\u001b[0m     )\n\u001b[1;32m    479\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    481\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:411\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    393\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    394\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    401\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    402\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself(\n\u001b[1;32m    403\u001b[0m         hidden_states,\n\u001b[1;32m    404\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m         output_attentions,\n\u001b[1;32m    410\u001b[0m     )\n\u001b[0;32m--> 411\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(self_outputs[\u001b[39m0\u001b[39;49m], hidden_states)\n\u001b[1;32m    412\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:363\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    361\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    362\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> 363\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLayerNorm(hidden_states \u001b[39m+\u001b[39;49m input_tensor)\n\u001b[1;32m    364\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[1;32m    191\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m~/repos/onenote-utils/.onenote/lib/python3.9/site-packages/torch/nn/functional.py:2511\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2507\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   2508\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2509\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   2510\u001b[0m     )\n\u001b[0;32m-> 2511\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\n",
    "    'mps')\n",
    "# activate training mode of model\n",
    "\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config.num_labels = max(category_map.values()) + 1\n",
    "model = BertForSequenceClassification(config).to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# begin training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_accm = 0\n",
    "    test_acc = 0\n",
    "    running_ex_size = 0\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # note that we move everything to the MPS device\n",
    "        batch_mps = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            'labels': batch['labels'].to(device)\n",
    "        }\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # train model on batch and return outputs (incl. loss)\n",
    "        outputs = model(**batch_mps)\n",
    "        # extract loss\n",
    "        loss = outputs[0]\n",
    "        loss_accm += loss.item()\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        running_ex_size += batch['input_ids'].shape[0]\n",
    "        test_acc += (\n",
    "            outputs[1].cpu().argmax(1) == batch['labels']).sum().item()\n",
    "        if i and (i % 100 == 0):\n",
    "            print(\n",
    "                f\"Epoch {epoch} Batch {i} accuracy {test_acc/running_ex_size}\")\n",
    "    print(f\"Epoch {epoch} loss: {loss_accm}\")\n",
    "    print(f\"Epoch {epoch} accuracy: {test_acc/len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_acc = 0\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        batch_mps = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            'labels': batch['labels'].to(device)\n",
    "        }\n",
    "        outputs = model(**batch_mps)\n",
    "        test_acc += (\n",
    "            outputs[1].cpu().argmax(1) == batch['labels']).sum().item()\n",
    "    print(f\"Test accuracy: {test_acc/len(val_dataset)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.onenote': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dacf5a04c83143f9e842fb8d3708c82df4a4fa4109d7b9c6a292db04d291cec5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
